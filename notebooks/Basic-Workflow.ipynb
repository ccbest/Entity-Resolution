{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637b5d61",
   "metadata": {},
   "source": [
    "### Welcome to ResolvER's workflow documentation!\n",
    "\n",
    "This notebook will give you a very basic understanding of a typical workflow you might encounter. We'll explore the following topics:\n",
    "* What is an Entlet?\n",
    "* Adding Entlets to an EntletMap\n",
    "* Creating resolution Strategies\n",
    "* Creating a Pipeline\n",
    "* Understanding Results\n",
    "\n",
    "Let's start by adding the project to your path so it's importable, and then importing the Entlet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b2da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')   # Assuming you `git clone`d the repo - append the library root to syspath for importing\n",
    "\n",
    "from resolver import Entlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e16ab",
   "metadata": {},
   "source": [
    "### What is an Entlet?\n",
    "\n",
    "An entlet is an entity according to a single source of information. It can be anything - a state, a person, a car...basically any noun.\n",
    "\n",
    "You can assign whatever properties you want to the entlet using the .add() method, and each property can contain as many values as you like. \"Nested\" values (typically in the form of dictionaries) and objects that contain one or more values, are respected, meaning the values will \"stay together\" throughout resolution.\n",
    "\n",
    "Certain properties (defined below in the Entlet IDs section) are \"reserved,\" meaning they will only accept a single value that cannot be changed/updated after it is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42eb9bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entlet()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entlet = Entlet()\n",
    "\n",
    "entlet.add({\n",
    "    \"name\": \"Lake County\",\n",
    "    \"location\": {\n",
    "        \"country\": \"US\",\n",
    "        \"state\": \"IL\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9947837",
   "metadata": {},
   "source": [
    "##### Working with Entlet IDs\n",
    "\n",
    "Entlet IDs are created for you, based on the information you added to the entlet. Generally, the goal of an Entlet ID is to:\n",
    "* Remain consistent across runs of the entity resolver\n",
    "* Uniquely identify that entlet within its source\n",
    "\n",
    "There are three ways an entlet can be created:\n",
    "\n",
    "**Defining a source unique id field** (Recommended)\n",
    "\n",
    "If your data source already supplies a unique id, you can specify the field name of the unique id. If you use this method, the field you define will become \"reserved,\" i.e., it will only accept a single value from the .add() method that cannot be overwritten.\n",
    "\n",
    "\n",
    "**Defining your own unique id** \n",
    "\n",
    "If no unique id is provided, you can define your own. This generally makes it a bit more difficult to keep the unique ids stable between runs of the entity resolver, so is not recommended.\n",
    "\n",
    "\n",
    "**Defining a combination of fields that uniquely identify the entlet**\n",
    "\n",
    "Alternatively, you can specify a combination of fields that together define the entlet as unique. These fields still allow multiple values; the values provided will be hashed together to create a the unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a98e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields required for unique ID creation: ['data_source', 'ent_type']\n",
      "The following fields behave as source-specific unique ID fields: countyFIPS\n"
     ]
    }
   ],
   "source": [
    "# Define a field (countyFIPS) that contains a unique id provided by the source (example dataset)\n",
    "# Note that this is a classmethod\n",
    "Entlet.define_source_uid_field(\"countyFIPS\")\n",
    "\n",
    "# Define your own unique id for this specific entlet\n",
    "# entlet.define_individual_id(1)\n",
    "\n",
    "# Define a combination of fields that uniquely identify the entlet\n",
    "# Entlet.define_custom_uid_fields(\"location.country\", \"location.state\"\", \"name\")\n",
    "\n",
    "print(\"Fields required for unique ID creation: \" + str(Entlet.UID_FIELDS))\n",
    "print(\"The following fields behave as source-specific unique ID fields: \" + str(Entlet.SOURCE_UID_FIELD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dae41b",
   "metadata": {},
   "source": [
    "You can add values to the entlet as you go.  Each field (except for \"reserved\" fields) is treated as a list, so new values are simply appended (duplicate values are discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e239fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entlet(countyFIPS=12345)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entlet.add({\n",
    "    \"countyFIPS\": \"12345\",  # Reserved field, because we defined it as the Source UID field\n",
    "    \"location\": {\n",
    "        \"country\": \"UK\",\n",
    "        \"state\": \"Not alabama\"\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95eef9",
   "metadata": {},
   "source": [
    "##### Reserved fields on the entlet\n",
    "\n",
    "You *must* add string values for the following 2 fields on every entlet:\n",
    "* ent_type\n",
    "* data_source\n",
    "\n",
    "Note that entlets with different ent_types *will not resolve together*. Capitalization matters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704cc87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entlet(countyFIPS=12345, ent_type=county, data_source=test)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entlet.add({\n",
    "    \"ent_type\": \"county\",\n",
    "    \"data_source\": \"test\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80288ab",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5809dee",
   "metadata": {},
   "source": [
    "### Creating an EntletMap\n",
    "\n",
    "Now that we've created our first entlet, added some data to it, and defined how it should produce its unique id, we have to add it to a \"pool\" of entlets for resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25d4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolver import EntletMap\n",
    "\n",
    "# We can simply instance EntletMap with one or more entlets,\n",
    "# but later we'll use the .add() method to add additional ones\n",
    "emap = EntletMap([entlet])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848dca22",
   "metadata": {},
   "source": [
    "We've added one entlet to the EntletMap. Now lets add a few more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc08a298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<resolver.EntletMap 3 entlets>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entlet = Entlet()\n",
    "entlet.add({\n",
    "    \"ent_type\": \"county\",\n",
    "    \"data_source\": \"test\",\n",
    "    \"countyFIPS\": \"23456\",\n",
    "    \"name\": \"Lake\",\n",
    "    \"location\": {\n",
    "        \"country\": \"US\",\n",
    "        \"state\": \"IL\"\n",
    "    }\n",
    "})\n",
    "emap.add(entlet)\n",
    "\n",
    "entlet = Entlet()\n",
    "entlet.add({\n",
    "    \"ent_type\": \"county\",\n",
    "    \"data_source\": \"test\",\n",
    "    \"countyFIPS\": \"34567\",\n",
    "    \"name\": \"DuPage County\",\n",
    "    \"location\": {\n",
    "        \"country\": \"US\",\n",
    "        \"state\": \"Illinois\"\n",
    "    }\n",
    "})\n",
    "emap.add(entlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f769fa",
   "metadata": {},
   "source": [
    "## Building a Resolution Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e618eba",
   "metadata": {},
   "source": [
    "#### Working with Standardizers\n",
    "\n",
    "We now have 3 Entlets in our EntletMap, but there's a problem. The last entlet we added has \"Illinois\" as its location.state value, while other the entlets have the value \"IL\". This could cause problems during resolution, so we should standardize the values before running the rest of the pipeline. Thankfully, the package provides a US State name-to-bigram standardizer.\n",
    "\n",
    "We only want to standardize instances where location.country is \"US,\" though; standardizing French states using American state names doesn't make any sense. So, we can apply a filter to say \"only standardize values in entlets where the 'country' value equals 'US' (more on standardization filters later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b4c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolver.standardizers import UsState2Code\n",
    "import operator\n",
    "\n",
    "state_std = UsState2Code(\n",
    "    \"location.state\",\n",
    "    filters=[{\n",
    "            \"field_name\": \"location.country\",\n",
    "            \"comparator\": operator.eq,\n",
    "            \"value\": \"US\"\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7a252",
   "metadata": {},
   "source": [
    "### Building Strategies\n",
    "\n",
    "Strategies are how you determine whether or not two entlets represent the same entity. A strategy can consist of as many ways to compare the entlets as you like, just remember that more strategies means:\n",
    "* More computation needed\n",
    "* Larger chance of \"bad\" resolutions occurring\n",
    "\n",
    "Strategies all consist of the following elements:\n",
    "* A 'blocker'\n",
    "* One or more ways to measure similarity between entlets\n",
    "* A way to determine whether the similarity scores pass a threshold test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78985ee",
   "metadata": {},
   "source": [
    "#### Defining a Blocker\n",
    "\n",
    "Blockers determine how entlets get \"paired up\" for comparison - which implies that *not all entlets will be compared against each other*. Entity Resolution is computationally expensive, and there's always a small chance that a \"bad\" resolution will occur. To mitigate both of these things, we define a blocker to try and pair up entlets in the smartest possible way.\n",
    "\n",
    "For this example, we'll use the SortedNeighborhood blocker, with a window size of 3. We tell the blocker to block based on the 'name' field, meaning the entlets will get sorted based on values in the 'name' field. The window size means that entlets will only be compared with their 3 closest alphabetical neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4cd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolver.blocking.sorted_value import SortedNeighborhood\n",
    "\n",
    "blocker = SortedNeighborhood(\"name\", window_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4780ff",
   "metadata": {},
   "source": [
    "#### Defining Transforms and Similarity Measures\n",
    "\n",
    "Now we need to determine how to compare values against each other. We'll define two similarity measures for this strategy:\n",
    "* Exact match between values of 'location.state'\n",
    "* Cosine similarity between the vector values of 'name'\n",
    "\n",
    "Note that the second strategy will require that we convert the value in the 'name' field to a vector. We can accomplish this using a transform that returns a vector. For this example, we'll use TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7eed678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolver.similarity import CosineSimilarity, ExactMatch\n",
    "from resolver.transforms import TfIdfTokenizedVector\n",
    "\n",
    "name_exact_match = ExactMatch(\"location.state\")\n",
    "\n",
    "tfidf = TfIdfTokenizedVector()\n",
    "name_tfidf_similarity = CosineSimilarity(\"name\", transforms=[tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070087e",
   "metadata": {},
   "source": [
    "#### Defining a scoring method\n",
    "\n",
    "Since we have two different measurements of similarity, we have to tell the Strategy how to combine those scores into a single heuristic that can be tested against a threshold we set.\n",
    "\n",
    "For this example, we'll treat each of the two scores as dimensions in a vector and treat that vector's magnitude as the overall score. Any combination of scores that produces a vector with magnitude greater than 1 will be considered a valid resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ace3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolver.scoring import VectorMagnitude\n",
    "\n",
    "scoring_method = VectorMagnitude(min=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2e1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resolver import Strategy\n",
    "\n",
    "# Roll up the above blocker, similarity metrics, and scoring method into a Strategy\n",
    "strategy = Strategy(\n",
    "    blocker=blocker,\n",
    "    metrics=[name_exact_match, name_tfidf_similarity],\n",
    "    scoring_method=scoring_method\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea5ed5",
   "metadata": {},
   "source": [
    "### Defining a Pipeline\n",
    "\n",
    "The Pipeline is what actually runs Entity Resolution, and will try to minimize the computational overhead of the strategies that you provide using under-the-hood optimizations. You can define as many strategies, standardizers, and partitioners (TODO) as you like.\n",
    "\n",
    "To run the pipeline against the entlets you've created, pass your EntletMap to the Pipeline's .resolve() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a2f461",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Lake County'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresolver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      3\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m      4\u001b[0m     strategies\u001b[38;5;241m=\u001b[39m[strategy], \n\u001b[1;32m      5\u001b[0m     standardizers\u001b[38;5;241m=\u001b[39m[state_std]\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m entity_map \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43memap\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/Malak/resolver/pipeline.py:31\u001b[0m, in \u001b[0;36mPipeline.resolve\u001b[0;34m(self, entletmap)\u001b[0m\n\u001b[1;32m     28\u001b[0m resolved_components\u001b[38;5;241m.\u001b[39madd_nodes_from(entletmap\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m strategy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategies:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mresolved_components\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentletmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentlet_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m entity_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conn_component \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mconnected_components(resolved_components):\n",
      "File \u001b[0;32m~/repos/Malak/venv/lib/python3.9/site-packages/networkx/classes/graph.py:933\u001b[0m, in \u001b[0;36mGraph.add_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_edges_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, ebunch_to_add, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;124;03m\"\"\"Add all the edges in ebunch_to_add.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m    >>> G.add_edges_from([(3, 4), (1, 4)], label=\"WN2898\")\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m ebunch_to_add:\n\u001b[1;32m    934\u001b[0m         ne \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(e)\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ne \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m~/repos/Malak/resolver/strategy.py:39\u001b[0m, in \u001b[0;36mStrategy.resolve\u001b[0;34m(self, entletmap, entlet_df)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m candidate_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocker\u001b[38;5;241m.\u001b[39mblock(entlet_df):\n\u001b[0;32m---> 39\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [metric\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m*\u001b[39m[entletmap\u001b[38;5;241m.\u001b[39mget(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m candidate_pair]) \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring_method(\u001b[38;5;241m*\u001b[39mscores):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m candidate_pair\n",
      "File \u001b[0;32m~/repos/Malak/resolver/strategy.py:39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m candidate_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocker\u001b[38;5;241m.\u001b[39mblock(entlet_df):\n\u001b[0;32m---> 39\u001b[0m     scores \u001b[38;5;241m=\u001b[39m [\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mentletmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcandidate_pair\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring_method(\u001b[38;5;241m*\u001b[39mscores):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m candidate_pair\n",
      "File \u001b[0;32m~/repos/Malak/resolver/similarity/vectors.py:27\u001b[0m, in \u001b[0;36mCosineSimilarity.run\u001b[0;34m(self, entlet1, entlet2)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, entlet1: Entlet, entlet2: Entlet) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity of 2 vectors.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        (float) the cosine similarity of the vectors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentlet1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentlet2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/Malak/resolver/similarity/vectors.py:28\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, entlet1: Entlet, entlet2: Entlet) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity of 2 vectors.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        (float) the cosine similarity of the vectors\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\n\u001b[0;32m---> 28\u001b[0m         \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entlet1\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield, [])\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m entlet2\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfield, [])\n\u001b[1;32m     31\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/Malak/venv/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1246\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1248\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/repos/Malak/venv/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    147\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    148\u001b[0m         X,\n\u001b[1;32m    149\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    165\u001b[0m         Y,\n\u001b[1;32m    166\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m~/repos/Malak/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:738\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    736\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    742\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Lake County'"
     ]
    }
   ],
   "source": [
    "from resolver import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    strategies=[strategy], \n",
    "    standardizers=[state_std]\n",
    ")\n",
    "\n",
    "entity_map = pipeline.resolve(emap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab56bb0",
   "metadata": {},
   "source": [
    "#### Working with Results\n",
    "\n",
    "The Pipeline will give you back an EntityMap, where the keys represent a unique Entity ID (as opposed to the earlier Ent*let* ID), and the values represent the aggregated information corresponding to all of the underlying entlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(entity_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035ddac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
